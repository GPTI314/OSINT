input {
  # File input for JSON logs
  file {
    path => "/usr/share/logstash/logs/osint-toolkit.json.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json
    type => "osint-json"
  }

  # File input for standard logs
  file {
    path => "/usr/share/logstash/logs/osint-toolkit.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "osint-standard"
  }

  # File input for error logs
  file {
    path => "/usr/share/logstash/logs/osint-toolkit.error.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json
    type => "osint-error"
  }

  # TCP input for real-time log streaming
  tcp {
    port => 5000
    codec => json_lines
    type => "osint-tcp"
  }
}

filter {
  # Parse timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }
  }

  # Add tags based on log level
  if [level] == "ERROR" or [level] == "CRITICAL" {
    mutate {
      add_tag => ["error"]
    }
  }

  if [level] == "WARNING" {
    mutate {
      add_tag => ["warning"]
    }
  }

  # Parse standard log format if not JSON
  if [type] == "osint-standard" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{DATA:logger_name} - %{LOGLEVEL:level} - %{GREEDYDATA:log_message}"
      }
    }
  }

  # Add environment information
  mutate {
    add_field => {
      "[@metadata][index]" => "osint-logs-%{+YYYY.MM.dd}"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "path"]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
    index => "%{[@metadata][index]}"
    document_type => "_doc"
  }

  # Output to stdout for debugging (comment out in production)
  stdout {
    codec => rubydebug
  }
}
